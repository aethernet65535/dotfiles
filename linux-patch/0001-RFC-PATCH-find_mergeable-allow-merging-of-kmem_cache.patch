From fc3e37ebc6139a072ae7d7724c9e2a33582efc1b Mon Sep 17 00:00:00 2001
From: aethernet <aethernet65535@gmail.com>
Date: Thu, 1 Jan 2026 02:00:11 +0800
Subject: [RFC PATCH] mm/slab: allow merging of caches with identical constructors

This is an experimental patch to allow merging of slab caches
that share the exact same constructor function. Currently, the
kernel prevents merging any caches with ctors, which might lead to
unnecessary duplication when the same initialization logic is used
across different caches.

Logic changes:
- Pass 'ctor' to slab_unmergeable() to ensure only
  caches with matching constructors are considered for merging.
- Adjust 'size' in find_mergeable() by sizeof(void *) when a ctor is
  present to stay consistent with the layout logic in calculate_sizes().

Test Status:
- Passed basic allocation and deallocation tests.
- Passed most common slab flag compatibility tests.

NOTE: This is an ongoing experiment by a beginner. More rigorous
stress testing is needed to identify potential edge cases or
regressions.

Disclaimer: This patch is for personal testing and experimental
purposes only. It is not intended for upstream submission (LKML).
The primary goal is to evaluate if allowing ctor-based merging
introduces new instabilities that weren't present in the original
slab logic.

Signed-off-by: aethernet <aethernet65535@gmail.com>
---
 mm/slab.h        | 2 +-
 mm/slab_common.c | 8 ++++----
 mm/slub.c        | 2 +-
 3 files changed, 6 insertions(+), 6 deletions(-)

diff --git a/mm/slab.h b/mm/slab.h
index e767aa7e91b0..361fef8146cf 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -399,7 +399,7 @@ extern void create_boot_cache(struct kmem_cache *, const char *name,
 			unsigned int size, slab_flags_t flags,
 			unsigned int useroffset, unsigned int usersize);
 
-int slab_unmergeable(struct kmem_cache *s);
+int slab_unmergeable(struct kmem_cache *s, void (*ctor)(void *));
 struct kmem_cache *find_mergeable(unsigned size, unsigned align,
 		slab_flags_t flags, const char *name, void (*ctor)(void *));
 struct kmem_cache *
diff --git a/mm/slab_common.c b/mm/slab_common.c
index eed7ea556cb1..edb601c19c43 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -150,12 +150,12 @@ static unsigned int calculate_alignment(slab_flags_t flags,
 /*
  * Find a mergeable slab cache
  */
-int slab_unmergeable(struct kmem_cache *s)
+int slab_unmergeable(struct kmem_cache *s, void (*ctor)(void *))
 {
 	if (slab_nomerge || (s->flags & SLAB_NEVER_MERGE))
 		return 1;
 
-	if (s->ctor)
+	if (s->ctor && likely(s->ctor != ctor))
 		return 1;
 
 #ifdef CONFIG_HARDENED_USERCOPY
@@ -184,7 +184,7 @@ struct kmem_cache *find_mergeable(unsigned int size, unsigned int align,
 		return NULL;
 
 	if (ctor)
-		return NULL;
+		size += sizeof(void *);
 
 	flags = kmem_cache_flags(flags, name);
 
@@ -196,7 +196,7 @@ struct kmem_cache *find_mergeable(unsigned int size, unsigned int align,
 	size = ALIGN(size, align);
 
 	list_for_each_entry_reverse(s, &slab_caches, list) {
-		if (slab_unmergeable(s))
+		if (slab_unmergeable(s, ctor))
 			continue;
 
 		if (size > s->size)
diff --git a/mm/slub.c b/mm/slub.c
index 861592ac5425..7c5faaf0bb2f 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -9734,7 +9734,7 @@ static int sysfs_slab_add(struct kmem_cache *s)
 	int err;
 	const char *name;
 	struct kset *kset = cache_kset(s);
-	int unmergeable = slab_unmergeable(s);
+	int unmergeable = slab_unmergeable(s, NULL);
 
 	if (!unmergeable && disable_higher_order_debug &&
 			(slub_debug & DEBUG_METADATA_FLAGS))
-- 
2.52.0

